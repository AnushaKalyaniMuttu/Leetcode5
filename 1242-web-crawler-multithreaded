// Interface provided by LeetCode:
interface HtmlParser {
    List<String> getUrls(String url);
}

class Solution {
    public List<String> crawl(String startUrl, HtmlParser htmlParser) {
        String host = getHost(startUrl);
        Set<String> visited = Collections.newSetFromMap(new ConcurrentHashMap<>());
        BlockingQueue<String> queue = new LinkedBlockingQueue<>();
        visited.add(startUrl);
        queue.add(startUrl);

        ExecutorService executor = Executors.newFixedThreadPool(4);
        CountDownLatch latch = new CountDownLatch(1);

        Runnable worker = () -> {
            try {
                while (true) {
                    String url = queue.poll(1, TimeUnit.SECONDS);
                    if (url == null) {
                        if (latch.getCount() == 0 && queue.isEmpty()) break;
                        continue;
                    }
                    for (String next : htmlParser.getUrls(url)) {
                        if (getHost(next).equals(host) && visited.add(next)) {
                            queue.add(next);
                        }
                    }
                }
            } catch (InterruptedException ignored) {
            }
        };

        // Start worker threads
        for (int i = 0; i < 4; i++) executor.submit(worker);
        latch.countDown(); // let workers know they can begin

        executor.shutdown();
        executor.awaitTermination(30, TimeUnit.SECONDS); // wait for finish

        return new ArrayList<>(visited);
    }

    private String getHost(String url) {
        int i = url.indexOf('/', 7);
        return i == -1 ? url : url.substring(0, i);
    }
}
